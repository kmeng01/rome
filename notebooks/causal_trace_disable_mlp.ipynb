{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8203f43f",
   "metadata": {},
   "source": [
    "# Frozen-MLP causal tracing\n",
    "\n",
    "This notebook executes causal traces with all the MLP modules for a token disabled, by freezing them at the corrupted state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ba3338",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "import torch, numpy\n",
    "import importlib, copy\n",
    "import transformers\n",
    "from collections import defaultdict\n",
    "from util import nethook\n",
    "from matplotlib import pyplot as plt\n",
    "from experiments.causal_trace import ModelAndTokenizer, make_inputs, predict_from_input, decode_tokens\n",
    "from experiments.causal_trace import layername, find_token_range, trace_with_patch, plot_trace_heatmap\n",
    "\n",
    "mt = ModelAndTokenizer('gpt2-xl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c18cfb8",
   "metadata": {},
   "source": [
    "## Tracing a single location\n",
    "\n",
    "The strategy here is to use three interventions, rather than two:\n",
    "\n",
    "1. As before, corrupt a subset of the input.\n",
    "2. As before, restore a subset of the internal hidden states to see\n",
    "   which ones restore the output.\n",
    "3. But now, while doing so, freeze a set of MLP modules when processing\n",
    "   the specific subject token, so that they are stuck in the corrupted\n",
    "   state.  This reveals effect of the hidden states on everything\n",
    "   except for those particular MLP executions.\n",
    "   \n",
    "This three-way intervention is implemented in `trace_with_repatch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36b314e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trace_with_repatch(\n",
    "    model,  # The model\n",
    "    inp,  # A set of inputs\n",
    "    states_to_patch,  # A list of (token index, layername) triples to restore\n",
    "    states_to_unpatch,  # A list of (token index, layername) triples to re-randomize\n",
    "    answers_t,  # Answer probabilities to collect\n",
    "    tokens_to_mix,  # Range of tokens to corrupt (begin, end)\n",
    "    noise=0.1,  # Level of noise to add\n",
    "):\n",
    "    prng = numpy.random.RandomState(1)  # For reproducibility, use pseudorandom noise\n",
    "    patch_spec = defaultdict(list)\n",
    "    for t, l in states_to_patch:\n",
    "        patch_spec[l].append(t)\n",
    "    unpatch_spec = defaultdict(list)\n",
    "    for t, l in states_to_unpatch:\n",
    "        unpatch_spec[l].append(t)\n",
    "\n",
    "    def untuple(x):\n",
    "        return x[0] if isinstance(x, tuple) else x\n",
    "\n",
    "    # Define the model-patching rule.\n",
    "    def patch_rep(x, layer):\n",
    "        if layer == \"transformer.wte\":\n",
    "            # If requested, we corrupt a range of token embeddings on batch items x[1:]\n",
    "            if tokens_to_mix is not None:\n",
    "                b, e = tokens_to_mix\n",
    "                x[1:, b:e] += noise * torch.from_numpy(\n",
    "                    prng.randn(x.shape[0] - 1, e - b, x.shape[2])\n",
    "                ).to(x.device)\n",
    "            return x\n",
    "        if first_pass or (layer not in patch_spec and layer not in unpatch_spec):\n",
    "            return x\n",
    "        # If this layer is in the patch_spec, restore the uncorrupted hidden state\n",
    "        # for selected tokens.\n",
    "        h = untuple(x)\n",
    "        for t in patch_spec.get(layer, []):\n",
    "            h[1:, t] = h[0, t]\n",
    "        for t in unpatch_spec.get(layer, []):\n",
    "            h[1:, t] = untuple(first_pass_trace[layer].output)[1:, t]\n",
    "        return x\n",
    "\n",
    "    # With the patching rules defined, run the patched model in inference.\n",
    "    for first_pass in [True, False] if states_to_unpatch else [False]:\n",
    "        with torch.no_grad(), nethook.TraceDict(\n",
    "            model,\n",
    "            [\"transformer.wte\"] + list(patch_spec.keys()) + list(unpatch_spec.keys()),\n",
    "            edit_output=patch_rep,\n",
    "        ) as td:\n",
    "            outputs_exp = model(**inp)\n",
    "            if first_pass:\n",
    "                first_pass_trace = td\n",
    "\n",
    "    # We report softmax probabilities for the answers_t token predictions of interest.\n",
    "    probs = torch.softmax(outputs_exp.logits[1:, -1, :], dim=1).mean(dim=0)[answers_t]\n",
    "\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe52a4a",
   "metadata": {},
   "source": [
    "## Tracing all locations\n",
    "\n",
    "Now we just need to repeat it over all locations, and draw the heatmaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9b5a7c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def calculate_hidden_flow_3(\n",
    "    mt, prompt, subject, token_range=None, samples=10, noise=0.1, window=10, disable_mlp=False, disable_attn=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs causal tracing over every token/layer combination in the network\n",
    "    and returns a dictionary numerically summarizing the results.\n",
    "    \"\"\"\n",
    "    inp = make_inputs(mt.tokenizer, [prompt] * (samples + 1))\n",
    "    with torch.no_grad():\n",
    "        answer_t, base_score = [d[0] for d in predict_from_input(mt.model, inp)]\n",
    "    [answer] = decode_tokens(mt.tokenizer, [answer_t])\n",
    "    e_range = find_token_range(mt.tokenizer, inp[\"input_ids\"][0], subject)\n",
    "    if token_range == 'last_subject':\n",
    "        token_range = [e_range[1] - 1]\n",
    "    low_score = trace_with_patch(mt.model, inp, [], answer_t, e_range,\n",
    "            noise=noise).item()\n",
    "    differences = trace_important_states_3(\n",
    "        mt.model, mt.num_layers, inp, e_range, answer_t, noise=noise,\n",
    "        disable_mlp=disable_mlp, disable_attn=disable_attn, token_range=token_range\n",
    "    )\n",
    "    differences = differences.detach().cpu()\n",
    "    return dict(\n",
    "        scores=differences,\n",
    "        low_score=low_score,\n",
    "        high_score=base_score,\n",
    "        input_ids=inp[\"input_ids\"][0],\n",
    "        input_tokens=decode_tokens(mt.tokenizer, inp[\"input_ids\"][0]),\n",
    "        subject_range=e_range,\n",
    "        answer=answer,\n",
    "        window=window,\n",
    "        kind=\"\",\n",
    "    )\n",
    "\n",
    "def trace_important_states_3(model, num_layers, inp, e_range, answer_t, noise=0.1, disable_mlp=False, disable_attn=False, token_range=None):\n",
    "    ntoks = inp[\"input_ids\"].shape[1]\n",
    "    table = []\n",
    "    zero_mlps = []\n",
    "    if token_range is None:\n",
    "        token_range = range(ntoks)\n",
    "    for tnum in token_range:\n",
    "        zero_mlps = []\n",
    "        if disable_mlp:\n",
    "            zero_mlps = [(tnum, layername(L, 'mlp')) for L in range(0, num_layers)]\n",
    "        if disable_attn:\n",
    "            zero_mlps += [(tnum, layername(L, 'attn')) for L in range(0, num_layers)]\n",
    "        row = []\n",
    "        for layer in range(0, num_layers):\n",
    "            r = trace_with_repatch(\n",
    "                model,\n",
    "                inp,\n",
    "                [(tnum, layername(layer))],\n",
    "                zero_mlps, # states_to_unpatch\n",
    "                answer_t,\n",
    "                tokens_to_mix=e_range,\n",
    "                noise=noise,\n",
    "            )\n",
    "            row.append(r)\n",
    "        table.append(torch.stack(row))\n",
    "    return torch.stack(table)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427f3989",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'Megan Rapinoe plays the sport of'\n",
    "entity = 'Megan Rapinoe'\n",
    "\n",
    "no_mlp_r = calculate_hidden_flow_3(mt, prefix, entity, disable_mlp=True)\n",
    "plot_trace_heatmap(no_mlp_r, title='Impact with MLP at last subject token disabled')\n",
    "ordinary_r = calculate_hidden_flow_3(mt, prefix, entity)\n",
    "plot_trace_heatmap(ordinary_r, title='Impact with MLP enabled as usual')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0362ea3",
   "metadata": {},
   "source": [
    "## Comparing the with-MLP and without-MLP traces\n",
    "\n",
    "Plotting on a bar graph makes it easier to see the difference between the causal effects with and without MLP enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2926aeac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_last_subject(mt, prefix, entity, token_range='last_subject', savepdf=None):\n",
    "    ordinary, no_mlp = calculate_last_subject(mt, prefix, entity, token_range=token_range)\n",
    "    plot_comparison(ordinary, no_mlp, prefix, savepdf=savepdf)\n",
    "\n",
    "def calculate_last_subject(mt, prefix, entity, cache=None, token_range='last_subject'):\n",
    "    def load_from_cache(filename):\n",
    "        try:\n",
    "            dat = numpy.load(f'{cache}/{filename}')\n",
    "            return {k: v if not isinstance(v, numpy.ndarray)\n",
    "                   else str(v) if v.dtype.type is numpy.str_\n",
    "                   else torch.from_numpy(v)\n",
    "                   for k, v in dat.items()}\n",
    "        except FileNotFoundError as e:\n",
    "            return None\n",
    "    no_mlp_r = load_from_cache('no_mlp.npz')\n",
    "    ordinary_r = load_from_cache('orindary.npz')\n",
    "    if no_mlp_r is None or ordinary_r is None:\n",
    "        no_mlp_r = calculate_hidden_flow_3(mt, prefix, entity, disable_mlp=True, token_range=token_range)\n",
    "        ordinary_r = calculate_hidden_flow_3(mt, prefix, entity, token_range=token_range)\n",
    "        if cache is not None:\n",
    "            os.makedirs(cache, exist_ok=True)\n",
    "            for r, filename in [(no_mlp_r, 'no_mlp.npz'), (ordinary_r, 'orindary.npz')]:\n",
    "                numpy.savez(f'{cache}/{filename}',\n",
    "                            **{k: v.cpu().numpy() if torch.is_tensor(v) else v for k, v in r.items()})\n",
    "    return ordinary_r['scores'][0] - ordinary_r['low_score'], no_mlp_r['scores'][0] - ordinary_r['low_score']\n",
    "\n",
    "def plot_comparison(ordinary, no_mlp, title, savepdf=None):\n",
    "    with plt.rc_context(rc={\"font.family\": \"Times New Roman\"}):\n",
    "        import matplotlib.ticker as mtick\n",
    "        fig, ax = plt.subplots(1, figsize=(6, 1.5), dpi=300)\n",
    "        ax.bar([i - 0.2 for i in range(48)], ordinary, width=0.4, color='#7261ab', label='Impact of single state on P')\n",
    "        ax.bar([i + 0.2 for i in range(48)], no_mlp, width=0.4, color='#f3701b', label='Impact with MLP disabled')\n",
    "        ax.set_title(title) #'Impact of individual hidden state at last subject token with MLP disabled')\n",
    "        ax.set_ylabel('Average Causal Effect')\n",
    "        #ax.set_xlabel('Layer at which the single hidden state is restored')\n",
    "        ax.yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "        ax.set_ylim(None, max(0.025, ordinary.max() * 1.05))\n",
    "        ax.legend()\n",
    "        if savepdf:\n",
    "            os.makedirs(os.path.dirname(savepdf), exist_ok=True)\n",
    "            plt.savefig(savepdf, bbox_inches=\"tight\")\n",
    "            plt.close()\n",
    "        else:\n",
    "            plt.show()\n",
    "\n",
    "if False: # Some representative cases.\n",
    "    plot_last_subject(mt, \"Megan Rapinoe plays the sport of\", \"Megan Rapinoe\")\n",
    "    plot_last_subject(mt, \"The Big Bang Theory premires on\", \"The Big Bang Theory\")\n",
    "    plot_last_subject(mt, \"Germaine Greer's domain of work is\", \"Germaine Greer\")\n",
    "    plot_last_subject(mt, \"Brian de Palma works in the area of\", \"Brian de Palma\")\n",
    "    plot_last_subject(mt, \"The headquarter of Zillow is in downtown\", \"Zillow\")\n",
    "    plot_last_subject(mt, \"Mitsubishi Electric started in the 1900s as a small company in\", \"Mitsubishi\")\n",
    "    plot_last_subject(mt, \"Mitsubishi Electric started in the 1900s as a small company in\", \"Mitsubishi Electric\")\n",
    "    plot_last_subject(mt, \"Madame de Montesson died in the city of\", \"Madame\")\n",
    "    plot_last_subject(mt, \"Madame de Montesson died in the city of\", \"Madame de Montesson\")\n",
    "    plot_last_subject(mt, \"Edmund Neupert, performing on the\", \"Edmund Neupert\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1631379e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_last_subject(mt, \"Megan Rapinoe plays the sport of\", \"Megan Rapinoe\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
